# PAI-Bench -- Predict

[![Python Version](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![Hugging Face Datasets](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Datasets-orange)](https://huggingface.co/datasets/shi-labs/physical-ai-bench-predict)

---

## Table of Contents

- [Dataset](#dataset)
- [Setup](#setup)
- [Usage](#usage)
  - [1. Prepare Your Result Videos](#1-prepare-your-result-videos)
  - [2. Run Video Quality Evaluation](#2-run-video-quality-evaluation)
  - [3. Run VLM Judge Evaluation](#3-run-vlm-judge-evaluation)
- [Acknowledgments](#acknowledgments)
- [Citation](#citation)

## Dataset

The full benchmark dataset is hosted on the Hugging Face Hub.

- **Hugging Face Link**: [physical-ai-bench-predict](https://huggingface.co/datasets/shi-labs/physical-ai-bench-predict)

## Setup

Follow these steps to set up the environment for running the benchmark.

### Environment Setup

```bash
git clone git@github.com:SHI-Labs/physical-ai-bench.git
cd physical-ai-bench/predict
```

### Install Dependencies

```bash
uv sync
uv pip install --no-build-isolation "git+https://github.com/facebookresearch/detectron2.git"
```

## Usage

### 1. Prepare Your Result Videos

Organize the videos generated by your models according to the following directory structure.

**Directory Structure:**

The `/path/to/your/videos` directory should contain video files directly:

```text
/path/to/your/videos/
├── {video_id}__{seed}.mp4
├── {video_id}__{seed}.mp4
└── ... (more video files following the naming pattern)
```

**File Naming Convention:**

- Each video file follows the pattern: `{video_id}__{seed}.mp4`
- The video_id represents a unique identifier for each video/prompt
- The seed represents different generated variations for the same video

### 2. Run Video Quality Evaluation

Use this command to evaluate video quality metrics including aesthetic quality, background consistency, imaging quality, motion smoothness, overall consistency, subject consistency, i2v background, and i2v subject:

```bash
uv run python -m torch.distributed.run --standalone --nproc_per_node 8 evaluate.py \
--mode custom_input \
--prompt_file ${path_to_hf_dataset}/cosmos_predict2_bench_full_info.json \
--custom_image_folder ${path_to_hf_dataset}/condition_image \
--dimension aesthetic_quality background_consistency imaging_quality motion_smoothness overall_consistency subject_consistency i2v_background i2v_subject \
--videos_path ${path_to_your_videos} \
--output_path ./evaluation_results/
```

**Argument Explanations:**

- `--mode custom_input`: Specifies the evaluation mode for custom input videos
- `--prompt_file`: Path to the benchmark prompt file from the downloaded dataset
- `--custom_image_folder`: Path to the condition images from the downloaded dataset
- `--dimension`: List of quality dimensions to evaluate
- `--videos_path`: Path to your organized video directory
- `--output_path`: Directory where evaluation results will be saved

### 3. Run VLM Judge Evaluation

Use this command to evaluate videos using Visual Language Model (VLM) based judging:

```bash
uv run python evaluate_vqa.py \
--prompt_file ${path_to_hf_dataset}/cosmos_predict2_bench_full_info.json \
--vqa_questions_dir ${path_to_hf_dataset}/vqa \
--videos_path ${path_to_your_videos} \
--output_path ./evaluation_results/
```

**Argument Explanations:**

- `--prompt_file`: Path to the benchmark prompt file from the downloaded dataset
- `--vqa_questions_dir`: Path to the VQA questions directory from the downloaded dataset
- `--videos_path`: Path to your organized video directory
- `--output_path`: Directory where evaluation results will be saved

## Acknowledgments

We would like to express our sincere gratitude to the developers and contributors of:

- [VBench](https://github.com/Vchitect/VBench) - The main reference repository for this evaluation framework

## Citation
